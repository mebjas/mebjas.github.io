---
layout: post
title: Adding attention mask in MultiHeadAttention layer of transformer using Keras and Tensorflow
categories: [AI, transformers, NLP]
description: ""
post-no: 45
toc: true
image: '../images/post45/post45_image1.png'
wip: true
---



{intro}.

## Attention Mask in transformer
TBA

## Adding attention masks to MultiHeadAttention layer using Keras & tensorflow
TBA

## References
TBA